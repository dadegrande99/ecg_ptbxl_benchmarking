{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils link\n",
    "\n",
    "- [Multi Label Model Evaluation](https://www.kaggle.com/code/kmkarakaya/multi-label-model-evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score, multilabel_confusion_matrix, classification_report, accuracy_score, jaccard_score, f1_score\n",
    "import os\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use([s for s in plt.style.available if 'whitegrid' in s][0])\n",
    "plt.rcParams['figure.figsize'] = [16, 9]\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_folders(path):\n",
    "    # Check if the path exists\n",
    "    if not os.path.exists(path):\n",
    "        return \"The specified path does not exist.\"\n",
    "\n",
    "    # Get a list of all items in the path\n",
    "    contents = os.listdir(path)\n",
    "\n",
    "    # Filter only the folders\n",
    "    folders = [item for item in contents if os.path.isdir(os.path.join(path, item))]\n",
    "\n",
    "    return folders\n",
    "\n",
    "def experiments(name:str):\n",
    "    experiments = [\n",
    "        ('exp0', 'all'),\n",
    "        ('exp1', 'diagnostic'),\n",
    "        ('exp1.1', 'subdiagnostic'),\n",
    "        ('exp1.1.1', 'superdiagnostic'),\n",
    "        ('exp2', 'form'),\n",
    "        ('exp3', 'rhythm')\n",
    "       ]\n",
    "    for exp in experiments:\n",
    "        if name == exp[0]:\n",
    "            return exp[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = {\n",
    "        'exp0': 'all',\n",
    "        'exp1': 'diagnostic',\n",
    "        'exp1.1': 'subdiagnostic',\n",
    "        'exp1.1.1': 'superdiagnostic',\n",
    "        'exp2': 'form',\n",
    "        'exp3': 'rhythm'\n",
    "       }\n",
    "\n",
    "datas = {\n",
    "\"test\": None, \n",
    "\"train\": None, \n",
    "\"val\": None\n",
    "}\n",
    "\n",
    "predictions = {\n",
    "\"test\": None, \n",
    "\"train\": None, \n",
    "\"val\": None\n",
    "}\n",
    "\n",
    "path = \"./output/\"\n",
    "\n",
    "threshold = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for exp in list_folders(path):\n",
    "    print(\"_\"*100)\n",
    "    print(exp)\n",
    "    for dt in datas:\n",
    "        datas[dt] = np.load(f'{path}/{exp}/data/y_{dt}.npy', allow_pickle=True)\n",
    "    for model in list_folders(f'{path}/{exp}/models'):\n",
    "        print(f'\\n\\t{model}')\n",
    "        for pred in predictions:\n",
    "            predictions[pred] = np.load(f'{path}/{exp}/models/{model}/y_{pred}_pred.npy', allow_pickle=True)\n",
    "            print(f'\\t\\t{pred}')\n",
    "            print(f'\\t\\t\\t- Shape: {predictions[pred].shape}')\n",
    "            print(f'\\t\\t\\t- Binary values: {len(np.unique(predictions[pred]))==2}')\n",
    "            print(f'\\t\\t\\t- Min value: {np.min(predictions[pred])}')\n",
    "            print(f'\\t\\t\\t- Max value: {np.max(predictions[pred])}')\n",
    "            print(f'\\t\\t\\t- Range 0-1: {(np.min(predictions[pred]) >= 0) and (np.max(predictions[pred]) <= 1)}')\n",
    "            \n",
    "\n",
    "\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "res = {}\n",
    "\n",
    "for exp in sorted(list_folders(path)):\n",
    "    res[exp] = {}\n",
    "    for dt in datas:\n",
    "        datas[dt] = np.load(f'{path}/{exp}/data/y_{dt}.npy', allow_pickle=True)\n",
    "    res[exp][\"data\"] = datas.copy()\n",
    "    for model in list_folders(f'{path}/{exp}/models'):\n",
    "        for pred in predictions:\n",
    "            predictions[pred] = np.load(f'{path}/{exp}/models/{model}/y_{pred}_pred.npy', allow_pickle=True)\n",
    "            res[exp][model]=predictions.copy()\n",
    "\n",
    "            # AUC score\n",
    "            results.append({\n",
    "                \"experiment\" : experiments[exp],\n",
    "                \"model\" : model,\n",
    "                \"metric\" : \"AUC\",\n",
    "                \"set\": pred,\n",
    "                \"value\": roc_auc_score(datas[pred], predictions[pred], average=\"weighted\")\n",
    "            })\n",
    "\n",
    "            # Accuracy score\n",
    "            results.append({\n",
    "                \"experiment\" : experiments[exp],\n",
    "                \"model\" : model,\n",
    "                \"metric\" : \"Accuracy\",\n",
    "                \"set\": pred,\n",
    "                \"value\": accuracy_score(datas[pred], (predictions[pred]>threshold))\n",
    "            })\n",
    "\n",
    "            # Jaccard score\n",
    "            results.append({\n",
    "                \"experiment\" : experiments[exp],\n",
    "                \"model\" : model,\n",
    "                \"metric\" : \"Jaccard\",\n",
    "                \"set\": pred,\n",
    "                \"value\": jaccard_score(datas[pred], (predictions[pred]>threshold), average=\"weighted\")\n",
    "            })\n",
    "\n",
    "            # F1-score\n",
    "            results.append({\n",
    "                \"experiment\" : experiments[exp],\n",
    "                \"model\" : model,\n",
    "                \"metric\" : \"F1\",\n",
    "                \"set\": pred,\n",
    "                \"value\": f1_score(datas[pred], (predictions[pred]>threshold), average=\"weighted\")\n",
    "            })\n",
    "\n",
    "            # Precision score\n",
    "            results.append({\n",
    "                \"experiment\" : experiments[exp],\n",
    "                \"model\" : model,\n",
    "                \"metric\" : \"Precision\",\n",
    "                \"set\": pred,\n",
    "                \"value\": metrics.precision_score(datas[pred], (predictions[pred]>threshold), average=\"weighted\")\n",
    "            })\n",
    "\n",
    "            # Recall score\n",
    "            results.append({\n",
    "                \"experiment\" : experiments[exp],\n",
    "                \"model\" : model,\n",
    "                \"metric\" : \"Recall\",\n",
    "                \"set\": pred,\n",
    "                \"value\": metrics.recall_score(datas[pred], (predictions[pred]>threshold), average=\"weighted\")\n",
    "            })\n",
    "\n",
    "results = pd.DataFrame(results)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[(results[\"experiment\"]==\"all\") & (results[\"metric\"]==\"AUC\") & (results[\"set\"]==\"test\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[(results[\"experiment\"]==\"diagnostic\") & (results[\"metric\"]==\"Accuracy\") & (results[\"set\"]==\"test\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline MI vs NORM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXP 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df = pd.read_csv(f'{path}../data/ptbxl/scp_statements.csv')\n",
    "print(agg_df.shape)\n",
    "agg_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df[agg_df.diagnostic_class == \"MI\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = \"exp0\"\n",
    "model = \"data\"\n",
    "y = []\n",
    "for set in res[exp][model]:\n",
    "    y.append(np.sum(res[exp][model][set], axis = 0))\n",
    "    #print(y[set].shape)\n",
    "y = np.array(y, dtype=int)\n",
    "y = np.sum(y, axis = 0)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elementi_diversi(array):\n",
    "    # Crea una lista vuota per tenere traccia degli elementi già visti\n",
    "    elementi_visti = []\n",
    "\n",
    "    # Itera attraverso gli elementi dell'array\n",
    "    for elemento in array:\n",
    "        # Se l'elemento è già presente nella lista, restituisci False\n",
    "        if elemento in elementi_visti:\n",
    "            print(f\"Elemento duplicato: {elemento}\")\n",
    "            return False\n",
    "        # Aggiungi l'elemento alla lista degli elementi visti\n",
    "        elementi_visti.append(elemento)\n",
    "\n",
    "    # Se il ciclo è completo senza restituire False, tutti gli elementi sono diversi\n",
    "    return True\n",
    "    \n",
    "\n",
    "elementi_diversi(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = {}\n",
    "diagnosis = {\n",
    "    \"MI\": None,\n",
    "    \"NORM\": None\n",
    "    }\n",
    "exp = \"exp0\"\n",
    "model = \"data\"\n",
    "\n",
    "# find diagnostic index\n",
    "dia_glob = []\n",
    "for dia in diagnosis:\n",
    "    diagnosis[dia] = agg_df.diagnostic_class == dia\n",
    "    diagnosis[dia] = np.where(diagnosis[dia])[0]\n",
    "    dia_glob.extend(diagnosis[dia])\n",
    "\n",
    "dia_glob = sorted(dia_glob)\n",
    "for dia in diagnosis:\n",
    "    diagnosis[dia] = [dia_glob.index(idx) for idx in diagnosis[dia]]\n",
    "\n",
    "\n",
    "\n",
    "y = {}\n",
    "for set in res[exp][model]:\n",
    "    y[set] = {}\n",
    "    for dia in diagnosis:\n",
    "        y[set][dia] = np.any((res[exp][model][set][:, agg_df.diagnostic_class == dia]), axis=1).astype(int)\n",
    "        print(f'{set} - {dia} {np.sum(y[set][dia])}/{len(y[set][dia])}')\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[exp][model][set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = {}\n",
    "diagnosis = {\n",
    "    \"MI\": None,\n",
    "    \"NORM\": None\n",
    "    }\n",
    "exp = \"exp0\"\n",
    "model = \"data\"\n",
    "\n",
    "# find diagnostic index\n",
    "dia_glob = []\n",
    "for dia in diagnosis:\n",
    "    diagnosis[dia] = agg_df.diagnostic_class == dia\n",
    "    diagnosis[dia] = np.where(diagnosis[dia])[0]\n",
    "    dia_glob.extend(diagnosis[dia])\n",
    "\n",
    "dia_glob = sorted(dia_glob)\n",
    "for dia in diagnosis:\n",
    "    diagnosis[dia] = [dia_glob.index(idx) for idx in diagnosis[dia]]\n",
    "\n",
    "# prepare baseline\n",
    "for model in res[exp]:\n",
    "    baseline[model] = {}\n",
    "    for set in res[exp][model]:\n",
    "        baseline[model][set] = res[exp][model][set][:, (agg_df.diagnostic_class.isin(diagnosis.keys()))]\n",
    "\n",
    "# prepare target\n",
    "y = baseline.pop(\"data\")\n",
    "for set in y:\n",
    "    tmp = []\n",
    "    for dia in diagnosis:\n",
    "        tmp.append(np.any(y[set][:, diagnosis[dia]], axis=1).astype(int))\n",
    "    y[set] = np.transpose(np.array(tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = \"data\"\n",
    "y = {}\n",
    "# prepare baseline\n",
    "for set in res[exp][model]:\n",
    "        y[set] = []\n",
    "        for dia in diagnosis:\n",
    "            y[set].append(np.any(res[exp][model][set][:, diagnosis[dia]], axis=1).astype(int))\n",
    "        y[set] = np.transpose(np.array(y[set]))\n",
    "# first count of elements\n",
    "counts = {'Dataset': [], 'NORM': [], 'MI': []}\n",
    "\n",
    "for dataset, values in y.items():\n",
    "    mi_count = np.sum(values[:, 0])\n",
    "    norm_count = np.sum(values[:, 1])\n",
    "    counts['Dataset'].append(dataset)\n",
    "    counts['NORM'].append(norm_count)\n",
    "    counts['MI'].append(mi_count)\n",
    "\n",
    "df2 = pd.DataFrame(counts)\n",
    "\n",
    "df2.set_index('Dataset', inplace=True)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = {}\n",
    "diagnosis = {\n",
    "    \"MI\": None,\n",
    "    \"NORM\": None\n",
    "    }\n",
    "exp = \"exp0\"\n",
    "\n",
    "# find diagnostic index\n",
    "dia_glob = []\n",
    "for dia in diagnosis:\n",
    "    diagnosis[dia] = agg_df.diagnostic_class == dia\n",
    "    diagnosis[dia] = np.where(diagnosis[dia])[0]\n",
    "    dia_glob.extend(diagnosis[dia])\n",
    "\n",
    "dia_glob = sorted(dia_glob)\n",
    "for dia in diagnosis:\n",
    "    diagnosis[dia] = [dia_glob.index(idx) for idx in diagnosis[dia]]\n",
    "\n",
    "# prepare baseline\n",
    "for model in res[exp]:\n",
    "    baseline[model] = {}\n",
    "    for set in res[exp][model]:\n",
    "        baseline[model][set] = res[exp][model][set][:, (agg_df.diagnostic_class.isin(diagnosis.keys()))]\n",
    "\n",
    "# prepare target\n",
    "y = baseline.pop(\"data\")\n",
    "for set in y:\n",
    "    tmp = []\n",
    "    for dia in diagnosis:\n",
    "        tmp.append(np.any(y[set][:, diagnosis[dia]], axis=1).astype(int))\n",
    "    y[set] = np.transpose(np.array(tmp))\n",
    "\n",
    "\n",
    "# find utils rows\n",
    "utils = {}\n",
    "for set in y:\n",
    "    utils[set] = np.sum(y[set], axis=1) == 1\n",
    "\n",
    "# prepare prediction values\n",
    "for model in baseline:\n",
    "    for set in baseline[model]:\n",
    "        baseline[model][set] = np.where(np.isin(np.argmax(baseline[model][set], axis = 1), diagnosis[\"MI\"]), 1, 0)\n",
    "\n",
    "# keep only utils rows in predictions\n",
    "for model in baseline:\n",
    "    for set in baseline[model]:\n",
    "        baseline[model][set] = baseline[model][set][utils[set]]\n",
    "\n",
    "# keep only utils rows in target & prepare it\n",
    "for set in y:\n",
    "    y[set] = y[set][utils[set]]\n",
    "    y[set] = y[set][:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"naive\"\n",
    "for set in baseline[model]:\n",
    "    print(f'{set} - {np.sum(baseline[model][set])} / {len(baseline[model][set])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for set in y:\n",
    "    print(f'{set} - {np.sum(y[set])} / {len(y[set])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_res = []\n",
    "\n",
    "for model in baseline:\n",
    "    for set in baseline[model]:\n",
    "\n",
    "        # AUC score\n",
    "        base_res.append({\n",
    "            \"model\" : model,\n",
    "            \"metric\" : \"AUC\",\n",
    "            \"set\": set,\n",
    "            \"value\": roc_auc_score(y[set], baseline[model][set], average=\"weighted\")\n",
    "        })\n",
    "\n",
    "        # Accuracy score\n",
    "        base_res.append({\n",
    "            \"model\" : model,\n",
    "            \"metric\" : \"Accuracy\",\n",
    "            \"set\": set,\n",
    "            \"value\": accuracy_score(y[set], baseline[model][set])\n",
    "        })\n",
    "\n",
    "        # Jaccard score\n",
    "        base_res.append({\n",
    "            \"model\" : model,\n",
    "            \"metric\" : \"Jaccard\",\n",
    "            \"set\": set,\n",
    "            \"value\": jaccard_score(y[set], baseline[model][set], average=\"weighted\")\n",
    "        })\n",
    "\n",
    "        # F1-score\n",
    "        base_res.append({\n",
    "            \"model\" : model,\n",
    "            \"metric\" : \"F1\",\n",
    "            \"set\": set,\n",
    "            \"value\": f1_score(y[set], baseline[model][set], average=\"weighted\")\n",
    "        })\n",
    "\n",
    "        # Precision score\n",
    "        base_res.append({\n",
    "            \"model\" : model,\n",
    "            \"metric\" : \"Precision\",\n",
    "            \"set\": set,\n",
    "            \"value\": metrics.precision_score(y[set], baseline[model][set], average=\"weighted\")\n",
    "        })\n",
    "\n",
    "        # Recall score\n",
    "        base_res.append({\n",
    "            \"model\" : model,\n",
    "            \"metric\" : \"Recall\",\n",
    "            \"set\": set,\n",
    "            \"value\": metrics.recall_score(y[set], baseline[model][set], average=\"weighted\")\n",
    "        })\n",
    "\n",
    "base_res = pd.DataFrame(base_res)\n",
    "base_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = {'Dataset': [], 'NORM': [], 'MI': []}\n",
    "\n",
    "for dataset, values in y.items():\n",
    "    norm_count = sum(values == 0)\n",
    "    mi_count = sum(values == 1)\n",
    "    counts['Dataset'].append(dataset)\n",
    "    counts['NORM'].append(norm_count)\n",
    "    counts['MI'].append(mi_count)\n",
    "\n",
    "df = pd.DataFrame(counts)\n",
    "\n",
    "df.set_index('Dataset', inplace=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(kind='bar', stacked=False, figsize=(10, 5), title='Class Distribution in the Datasets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = {}\n",
    "diagnosis_complete = {\n",
    "    'MI' :{\n",
    "        'IMI': 18,\n",
    "        'ASMI': 9,\n",
    "        'ILMI': 17,\n",
    "        'AMI': 7,\n",
    "        'ALMI': 6,\n",
    "        'INJAS': 20,\n",
    "        'LMI': 38,\n",
    "        'INJAL': 19,\n",
    "        'IPLMI': 25,\n",
    "        'IPMI': 26,\n",
    "        'INJIN': 22,\n",
    "        'INJLA': 23,\n",
    "        'PMI': 23,\n",
    "        'INJIL': 21\n",
    "    }, 'NORM': {\n",
    "        'NORM': 46\n",
    "    }}\n",
    "exp = \"exp0\"\n",
    "\n",
    "# find diagnostic index\n",
    "diagnosis = {}\n",
    "dia_glob = []\n",
    "for dia in diagnosis_complete:\n",
    "    diagnosis[dia] = list(diagnosis_complete[dia].values())\n",
    "    dia_glob.extend(diagnosis[dia])\n",
    "\n",
    "dia_glob = sorted(dia_glob)\n",
    "for dia in diagnosis:\n",
    "    diagnosis[dia] = np.array([dia_glob.index(idx) for idx in diagnosis[dia]], dtype=np.int8)\n",
    "\n",
    "# prepare baseline\n",
    "for model in res[exp]:\n",
    "    baseline[model] = {}\n",
    "    for set in res[exp][model]:\n",
    "        baseline[model][set] = res[exp][model][set][:, dia_glob]\n",
    "\n",
    "# prepare target\n",
    "y = baseline.pop(\"data\")\n",
    "for set in y:\n",
    "    tmp = []\n",
    "    for dia in diagnosis:\n",
    "        tmp.append(np.any(y[set][:, diagnosis[dia]], axis=1).astype(int))\n",
    "    y[set] = np.transpose(np.array(tmp))\n",
    "\n",
    "\n",
    "# find utils rows\n",
    "utils = {}\n",
    "for set in y:\n",
    "    utils[set] = np.sum(y[set], axis=1) == 1\n",
    "\n",
    "# prepare prediction values\n",
    "for model in baseline:\n",
    "    for set in baseline[model]:\n",
    "        baseline[model][set] = np.where(np.isin(np.argmax(baseline[model][set], axis = 1), diagnosis[\"MI\"]), 1, 0)\n",
    "\n",
    "# keep only utils rows in predictions\n",
    "for model in baseline:\n",
    "    for set in baseline[model]:\n",
    "        baseline[model][set] = baseline[model][set][utils[set]]\n",
    "\n",
    "# keep only utils rows in target & prepare it\n",
    "for set in y:\n",
    "    y[set] = y[set][utils[set]]\n",
    "    y[set] = y[set][:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = {'Dataset': [], 'NORM': [], 'MI': []}\n",
    "\n",
    "for dataset, values in y.items():\n",
    "    norm_count = sum(values == 0)\n",
    "    mi_count = sum(values == 1)\n",
    "    counts['Dataset'].append(dataset)\n",
    "    counts['NORM'].append(norm_count)\n",
    "    counts['MI'].append(mi_count)\n",
    "\n",
    "df = pd.DataFrame(counts)\n",
    "\n",
    "df.set_index('Dataset', inplace=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(kind='bar', stacked=False, figsize=(10, 5), title='Class Distribution in the Datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_res = []\n",
    "\n",
    "for model in baseline:\n",
    "    for set in baseline[model]:\n",
    "\n",
    "        # AUC score\n",
    "        base_res.append({\n",
    "            \"model\" : model,\n",
    "            \"metric\" : \"AUC\",\n",
    "            \"set\": set,\n",
    "            \"value\": roc_auc_score(y[set], baseline[model][set], average=\"weighted\")\n",
    "        })\n",
    "\n",
    "        # Accuracy score\n",
    "        base_res.append({\n",
    "            \"model\" : model,\n",
    "            \"metric\" : \"Accuracy\",\n",
    "            \"set\": set,\n",
    "            \"value\": accuracy_score(y[set], baseline[model][set])\n",
    "        })\n",
    "\n",
    "        # Jaccard score\n",
    "        base_res.append({\n",
    "            \"model\" : model,\n",
    "            \"metric\" : \"Jaccard\",\n",
    "            \"set\": set,\n",
    "            \"value\": jaccard_score(y[set], baseline[model][set], average=\"weighted\")\n",
    "        })\n",
    "\n",
    "        # F1-score\n",
    "        base_res.append({\n",
    "            \"model\" : model,\n",
    "            \"metric\" : \"F1\",\n",
    "            \"set\": set,\n",
    "            \"value\": f1_score(y[set], baseline[model][set], average=\"weighted\")\n",
    "        })\n",
    "\n",
    "        # Precision score\n",
    "        base_res.append({\n",
    "            \"model\" : model,\n",
    "            \"metric\" : \"Precision\",\n",
    "            \"set\": set,\n",
    "            \"value\": metrics.precision_score(y[set], baseline[model][set], average=\"weighted\")\n",
    "        })\n",
    "\n",
    "        # Recall score\n",
    "        base_res.append({\n",
    "            \"model\" : model,\n",
    "            \"metric\" : \"Recall\",\n",
    "            \"set\": set,\n",
    "            \"value\": metrics.recall_score(y[set], baseline[model][set], average=\"weighted\")\n",
    "        })\n",
    "\n",
    "base_res = pd.DataFrame(base_res)\n",
    "base_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_res[(base_res[\"metric\"]==\"AUC\") & (base_res[\"set\"]==\"test\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set =  \"test\"\n",
    "model = \"data\"\n",
    "\n",
    "for exp in res:\n",
    "    print(f'labels for {exp} - {res[exp][model][set].shape[0]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env_name",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
