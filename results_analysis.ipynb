{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils link\n",
    "\n",
    "- [Multi Label Model Evaluation](https://www.kaggle.com/code/kmkarakaya/multi-label-model-evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score, multilabel_confusion_matrix, classification_report, accuracy_score, jaccard_score, f1_score\n",
    "import os\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sys import path\n",
    "import ast\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "path.append(\"./code/\")\n",
    "from utils.utils import compute_label_aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use([s for s in plt.style.available if 'whitegrid' in s][0])\n",
    "plt.rcParams['figure.figsize'] = [16, 9]\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_folders(path):\n",
    "    # Check if the path exists\n",
    "    if not os.path.exists(path):\n",
    "        return \"The specified path does not exist.\"\n",
    "\n",
    "    # Get a list of all items in the path\n",
    "    contents = os.listdir(path)\n",
    "\n",
    "    # Filter only the folders\n",
    "    folders = [item for item in contents if os.path.isdir(os.path.join(path, item))]\n",
    "\n",
    "    return folders\n",
    "\n",
    "def experiments(name:str):\n",
    "    experiments = [\n",
    "        ('exp0', 'all'),\n",
    "        ('exp1', 'diagnostic'),\n",
    "        ('exp1.1', 'subdiagnostic'),\n",
    "        ('exp1.1.1', 'superdiagnostic'),\n",
    "        ('exp2', 'form'),\n",
    "        ('exp3', 'rhythm')\n",
    "       ]\n",
    "    for exp in experiments:\n",
    "        if name == exp[0]:\n",
    "            return exp[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = {\n",
    "        'exp0': 'all',\n",
    "        'exp1': 'diagnostic',\n",
    "        'exp1.1': 'subdiagnostic',\n",
    "        'exp1.1.1': 'superdiagnostic',\n",
    "        'exp2': 'form',\n",
    "        'exp3': 'rhythm'\n",
    "       }\n",
    "\n",
    "datas = {\n",
    "\"test\": None, \n",
    "\"train\": None, \n",
    "\"val\": None\n",
    "}\n",
    "\n",
    "predictions = {\n",
    "\"test\": None, \n",
    "\"train\": None, \n",
    "\"val\": None\n",
    "}\n",
    "\n",
    "path_out = \"./output/\"\n",
    "\n",
    "threshold = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for exp in list_folders(path_out):\n",
    "    print(\"_\"*100)\n",
    "    print(exp)\n",
    "    for dt in datas:\n",
    "        datas[dt] = np.load(f'{path_out}/{exp}/data/y_{dt}.npy', allow_pickle=True)\n",
    "    for model in list_folders(f'{path_out}/{exp}/models'):\n",
    "        print(f'\\n\\t{model}')\n",
    "        for pred in predictions:\n",
    "            predictions[pred] = np.load(f'{path_out}/{exp}/models/{model}/y_{pred}_pred.npy', allow_pickle=True)\n",
    "            print(f'\\t\\t{pred}')\n",
    "            print(f'\\t\\t\\t- Shape: {predictions[pred].shape}')\n",
    "            print(f'\\t\\t\\t- Binary values: {len(np.unique(predictions[pred]))==2}')\n",
    "            print(f'\\t\\t\\t- Min value: {np.min(predictions[pred])}')\n",
    "            print(f'\\t\\t\\t- Max value: {np.max(predictions[pred])}')\n",
    "            print(f'\\t\\t\\t- Range 0-1: {(np.min(predictions[pred]) >= 0) and (np.max(predictions[pred]) <= 1)}')\n",
    "            \n",
    "\n",
    "\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "res = {}\n",
    "\n",
    "for exp in sorted(list_folders(path_out)):\n",
    "    res[exp] = {}\n",
    "    for dt in datas:\n",
    "        datas[dt] = np.load(f'{path_out}/{exp}/data/y_{dt}.npy', allow_pickle=True)\n",
    "    res[exp][\"data\"] = datas.copy()\n",
    "    for model in list_folders(f'{path_out}/{exp}/models'):\n",
    "        for pred in predictions:\n",
    "            predictions[pred] = np.load(f'{path_out}/{exp}/models/{model}/y_{pred}_pred.npy', allow_pickle=True)\n",
    "            res[exp][model]=predictions.copy()\n",
    "\n",
    "            # AUC score\n",
    "            results.append({\n",
    "                \"experiment\" : experiments[exp],\n",
    "                \"model\" : model,\n",
    "                \"metric\" : \"AUC\",\n",
    "                \"set\": pred,\n",
    "                \"value\": roc_auc_score(datas[pred], predictions[pred], average=\"weighted\")\n",
    "            })\n",
    "\n",
    "            # Accuracy score\n",
    "            results.append({\n",
    "                \"experiment\" : experiments[exp],\n",
    "                \"model\" : model,\n",
    "                \"metric\" : \"Accuracy\",\n",
    "                \"set\": pred,\n",
    "                \"value\": accuracy_score(datas[pred], (predictions[pred]>threshold))\n",
    "            })\n",
    "\n",
    "            # Jaccard score\n",
    "            results.append({\n",
    "                \"experiment\" : experiments[exp],\n",
    "                \"model\" : model,\n",
    "                \"metric\" : \"Jaccard\",\n",
    "                \"set\": pred,\n",
    "                \"value\": jaccard_score(datas[pred], (predictions[pred]>threshold), average=\"weighted\")\n",
    "            })\n",
    "\n",
    "            # F1-score\n",
    "            results.append({\n",
    "                \"experiment\" : experiments[exp],\n",
    "                \"model\" : model,\n",
    "                \"metric\" : \"F1\",\n",
    "                \"set\": pred,\n",
    "                \"value\": f1_score(datas[pred], (predictions[pred]>threshold), average=\"weighted\")\n",
    "            })\n",
    "\n",
    "            # Precision score\n",
    "            results.append({\n",
    "                \"experiment\" : experiments[exp],\n",
    "                \"model\" : model,\n",
    "                \"metric\" : \"Precision\",\n",
    "                \"set\": pred,\n",
    "                \"value\": metrics.precision_score(datas[pred], (predictions[pred]>threshold), average=\"weighted\")\n",
    "            })\n",
    "\n",
    "            # Recall score\n",
    "            results.append({\n",
    "                \"experiment\" : experiments[exp],\n",
    "                \"model\" : model,\n",
    "                \"metric\" : \"Recall\",\n",
    "                \"set\": pred,\n",
    "                \"value\": metrics.recall_score(datas[pred], (predictions[pred]>threshold), average=\"weighted\")\n",
    "            })\n",
    "\n",
    "results = pd.DataFrame(results)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[(results[\"experiment\"]==\"all\") & (results[\"metric\"]==\"AUC\") & (results[\"set\"]==\"test\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[(results[\"experiment\"]==\"diagnostic\") & (results[\"metric\"]==\"Accuracy\") & (results[\"set\"]==\"test\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline MI vs NORM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df = pd.read_csv(f'{path_out}/../data/ptbxl/scp_statements.csv', index_col=0)\n",
    "print(agg_df.shape)\n",
    "agg_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df[agg_df.diagnostic_class == \"MI\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df[agg_df.diagnostic_class == \"NORM\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXP 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "Y: pd.DataFrame = pd.read_csv('./data/ptbxl/ptbxl_database.csv', index_col='ecg_id')\n",
    "Y['scp_codes'] = Y['scp_codes'].apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "Y_clean: pd.DataFrame = compute_label_aggregations(Y, \"./data/ptbxl/\", 'all')\n",
    "\n",
    "counts = pd.Series(np.concatenate(Y_clean['all_scp'].values)).value_counts()\n",
    "Y_clean['all_scp'] = Y_clean['all_scp'].apply(lambda x: list(set(x).intersection(set(counts.index.values))))\n",
    "Y_clean['all_scp_len'] = Y_clean['all_scp'].apply(lambda x: len(x))\n",
    "\n",
    "# select\n",
    "Y = Y_clean[Y_clean['all_scp_len'] > 0]\n",
    "mlb.fit(Y['all_scp'].values)\n",
    "y = mlb.transform(Y['all_scp'].values)\n",
    "\n",
    "diagnosis_complete = {\n",
    "    \"MI\" : None,\n",
    "    \"NORM\" : None\n",
    "}\n",
    "\n",
    "for dia in diagnosis_complete:\n",
    "    diagnosis_complete[dia] = {el: np.where(mlb.classes_ == el)[0][0] for el in agg_df[agg_df.diagnostic_class == dia].index}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = {}\n",
    "exp = \"exp0\"\n",
    "\n",
    "# find diagnostic index\n",
    "diagnosis = {}\n",
    "dia_glob = []\n",
    "for dia in diagnosis_complete:\n",
    "    diagnosis[dia] = list(diagnosis_complete[dia].values())\n",
    "    dia_glob.extend(diagnosis[dia])\n",
    "\n",
    "dia_glob = sorted(dia_glob)\n",
    "for dia in diagnosis:\n",
    "    diagnosis[dia] = np.array([dia_glob.index(idx) for idx in diagnosis[dia]], dtype=np.int8)\n",
    "\n",
    "# prepare baseline\n",
    "for model in res[exp]:\n",
    "    baseline[model] = {}\n",
    "    for set in res[exp][model]:\n",
    "        baseline[model][set] = res[exp][model][set][:, dia_glob]\n",
    "\n",
    "# prepare target\n",
    "y = baseline.pop(\"data\")\n",
    "for set in y:\n",
    "    tmp = []\n",
    "    for dia in diagnosis:\n",
    "        tmp.append(np.any(y[set][:, diagnosis[dia]], axis=1).astype(int))\n",
    "    y[set] = np.transpose(np.array(tmp))\n",
    "\n",
    "\n",
    "# find utils rows\n",
    "utils = {}\n",
    "for set in y:\n",
    "    utils[set] = np.sum(y[set], axis=1) == 1\n",
    "\n",
    "# prepare prediction values\n",
    "for model in baseline:\n",
    "    for set in baseline[model]:\n",
    "        baseline[model][set] = np.where(np.isin(np.argmax(baseline[model][set], axis = 1), diagnosis[\"MI\"]), 1, 0)\n",
    "\n",
    "# keep only utils rows in predictions\n",
    "for model in baseline:\n",
    "    for set in baseline[model]:\n",
    "        baseline[model][set] = baseline[model][set][utils[set]]\n",
    "\n",
    "# keep only utils rows in target & prepare it\n",
    "for set in y:\n",
    "    y[set] = y[set][utils[set]]\n",
    "    y[set] = y[set][:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = {'Dataset': [], 'NORM': [], 'MI': []}\n",
    "\n",
    "for dataset, values in y.items():\n",
    "    norm_count = sum(values == 0)\n",
    "    mi_count = sum(values == 1)\n",
    "    counts['Dataset'].append(dataset)\n",
    "    counts['NORM'].append(norm_count)\n",
    "    counts['MI'].append(mi_count)\n",
    "\n",
    "df = pd.DataFrame(counts)\n",
    "df.set_index('Dataset', inplace=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(kind='bar', stacked=False, figsize=(10, 5), title='Class Distribution in the Datasets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_res = []\n",
    "\n",
    "for model in baseline:\n",
    "    for set in baseline[model]:\n",
    "\n",
    "        # AUC score\n",
    "        base_res.append({\n",
    "            \"model\" : model,\n",
    "            \"metric\" : \"AUC\",\n",
    "            \"set\": set,\n",
    "            \"value\": roc_auc_score(y[set], baseline[model][set], average=\"weighted\")\n",
    "        })\n",
    "\n",
    "        # Accuracy score\n",
    "        base_res.append({\n",
    "            \"model\" : model,\n",
    "            \"metric\" : \"Accuracy\",\n",
    "            \"set\": set,\n",
    "            \"value\": accuracy_score(y[set], baseline[model][set])\n",
    "        })\n",
    "\n",
    "        # Jaccard score\n",
    "        base_res.append({\n",
    "            \"model\" : model,\n",
    "            \"metric\" : \"Jaccard\",\n",
    "            \"set\": set,\n",
    "            \"value\": jaccard_score(y[set], baseline[model][set], average=\"weighted\")\n",
    "        })\n",
    "\n",
    "        # F1-score\n",
    "        base_res.append({\n",
    "            \"model\" : model,\n",
    "            \"metric\" : \"F1\",\n",
    "            \"set\": set,\n",
    "            \"value\": f1_score(y[set], baseline[model][set], average=\"weighted\")\n",
    "        })\n",
    "\n",
    "        # Precision score\n",
    "        base_res.append({\n",
    "            \"model\" : model,\n",
    "            \"metric\" : \"Precision\",\n",
    "            \"set\": set,\n",
    "            \"value\": metrics.precision_score(y[set], baseline[model][set], average=\"weighted\")\n",
    "        })\n",
    "\n",
    "        # Recall score\n",
    "        base_res.append({\n",
    "            \"model\" : model,\n",
    "            \"metric\" : \"Recall\",\n",
    "            \"set\": set,\n",
    "            \"value\": metrics.recall_score(y[set], baseline[model][set], average=\"weighted\")\n",
    "        })\n",
    "\n",
    "base_res = pd.DataFrame(base_res)\n",
    "base_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_res.metric.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_res[(base_res[\"metric\"]==\"Recall\") & (base_res[\"set\"]==\"test\")].sort_values(by=\"model\", ascending=False)[[\"model\", \"value\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set =  \"test\"\n",
    "model = \"data\"\n",
    "\n",
    "for exp in res:\n",
    "    print(f'labels for {exp} - {res[exp][model][set].shape[0]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env_name",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
